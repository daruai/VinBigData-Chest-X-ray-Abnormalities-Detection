{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH_INPUT = \"../input/vinbigdata-chest-xray-abnormalities-detection/\"\ndf_labels=pd.read_csv(PATH_INPUT+\"train.csv\")\ndf_labels[\"path\"]=df_labels.image_id.apply(lambda x: PATH_INPUT+f\"train/{x}.dicom\")\ndf_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_labels.class_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_labels.image_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of regions per image\ndf_labels.query(\"class_id!=14\").image_id.value_counts().plot.hist(bins=50,log=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Colormap\nlabels_info=df_labels.query(\"class_id!=14\")[[\"class_id\",\"class_name\"]].drop_duplicates().sort_values(by=\"class_id\").reset_index(drop=True)\nlabels_info[\"color\"]=sns.color_palette(\"husl\", labels.shape[0])\nlabels_info[\"color\"]=labels_info[\"color\"].apply(lambda x: tuple([int(cl*255) for cl in x]))\nlabels_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reading a dicom file\n# From: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom skimage import exposure\nimport cv2\n\ndef dicom2array(path, voi_lut=True, fix_monochrome=True, equalize_hist=False):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    \n    if equalize_hist:\n        data = exposure.equalize_hist(data)\n        \n    return data\n\ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n    \ndef plot_img_with_labels(image_id,df_labels, labels_info, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    df_plot = df_labels.query(f\"image_id=='{image_id}'\")\n    img=dicom2array(df_plot.iloc[0].path)\n    \n    # To rgb:\n    img=cv2.merge((img, img, img))\n    # Draw boxes if present\n    df_boxes = df_plot.query(\"class_id!=14\")\n    if df_boxes.shape[0]>0:\n        for i,row in df_boxes.iterrows(): #x_min\ty_min\tx_max\ty_max\n            img = cv2.rectangle(img,(int(row.x_min), int(row.y_min)),(int(row.x_max), int(row.y_max)), labels_info.loc[row.class_id][\"color\"],5)\n            \n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_labels.query(\"image_id=='9a5094b2563a1ef3ff50dc5c7ff71345'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nfor img_id in random.choices(df_labels.image_id.unique(),k=10):\n    plot_img_with_labels(img_id,df_labels, labels_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_example=df_labels.loc[0].path\nprint(path_example)\ndicom2array(path_example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate images in png format\nimgs_to_process=df_labels.image_id.unique()\n\ninput_paths = [f\"../input/vinbigdata-chest-xray-abnormalities-detection/train/{x}.dicom\" for x in paths_to_process] \nos.mkdir(\"./preprocessed/\")\noutput_paths= [f\"./preprocessed/{x}.png\" for x in imgs_to_process]\n\n\nfor inp,out in tqdm(zip(input_paths,output_paths)):\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paths_to_process[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\ngit clone https://github.com/ahmedfgad/Mask-RCNN-TF2.git\nmv -f Mask-RCNN-TF2/* .\nrm -rf Mask-RCNN-TF2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log\n\nROOT_DIR=\"./\"\n# Directory to save logs and trained model\nMODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n\n# Local path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n# Download COCO trained weights from Releases if needed\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class XrayConfig(Config):\n    \"\"\"Configuration for training on the toy shapes dataset.\n    Derives from the base Config class and overrides values specific\n    to the toy shapes dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"xray\"\n\n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + df_labels.shape[0]  # background + labels\n\n    # Use small images for faster training. Set the limits of the small side\n    # the large side, and that determines the image shape.\n    IMAGE_MIN_DIM = 128\n    IMAGE_MAX_DIM = 128\n\n    # Use smaller anchors because our image and objects are small\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n\n    # Reduce training ROIs per image because the images are small and have\n    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n    TRAIN_ROIS_PER_IMAGE = 32\n\n    # Use a small epoch since the data is simple\n    STEPS_PER_EPOCH = 1000\n\n    # use small validation steps since the epoch is small\n    VALIDATION_STEPS = 500\n    \nconfig = XrayConfig()\nconfig.display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}